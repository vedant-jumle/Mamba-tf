{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.set_visible_devices(gpus[1], 'GPU')\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from einops import rearrange, repeat\n",
    "from typing import Union\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import datasets\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    model_input_dims: int = 64\n",
    "    model_states: int = 64\n",
    "    projection_expand_factor: int = 2\n",
    "    conv_kernel_size: int = 4\n",
    "    delta_t_min: float = 0.001\n",
    "    delta_t_max: float = 0.1\n",
    "    delta_t_scale: float = 0.1\n",
    "    delta_t_init_floor: float = 1e-4\n",
    "    conv_use_bias: bool = True\n",
    "    dense_use_bias: bool = False\n",
    "    layer_id: int = -1\n",
    "    seq_length: int = 128\n",
    "    num_layers: int = 5\n",
    "    dropout_rate: float = 0.2\n",
    "    use_lm_head: float = False\n",
    "    num_classes: int = None\n",
    "    vocab_size: int = None\n",
    "    final_activation = None\n",
    "    loss:Union[str, keras.losses.Loss] = None\n",
    "    optimizer: Union[str, keras.optimizers.Optimizer] = keras.optimizers.AdamW()\n",
    "    metrics = ['accuracy']\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.model_internal_dim: int = int(self.projection_expand_factor * self.model_input_dims)\n",
    "\n",
    "        self.delta_t_rank = math.ceil(self.model_input_dims/16)\n",
    "        if self.layer_id == -1:\n",
    "            self.layer_id = np.round(np.random.randint(0, 1000), 4)\n",
    "\n",
    "        if self.vocab_size == None:\n",
    "            raise ValueError(\"vocab size cannot be none\")\n",
    "\n",
    "        if self.use_lm_head:\n",
    "            self.num_classes=self.vocab_size\n",
    "        else:\n",
    "            if self.num_classes == None:\n",
    "                raise ValueError(f'num classes cannot be {self.num_classes}')\n",
    "\n",
    "            if self.num_classes == 1:\n",
    "                self.final_activation = 'sigmoid'\n",
    "            else:\n",
    "                self.final_activation = 'softmax'\n",
    "\n",
    "        if self.loss == None:\n",
    "            raise ValueError(f\"loss cannot be {self.loss}\")\n",
    "        \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not used in the code, but can be used, working\n",
    "class RMSNorm(layers.Layer):\n",
    "    def __init__(self, d_model: int, eps: float=1e-5, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.eps = eps\n",
    "        self.weight = tf.Variable(np.ones(d_model), dtype=tf.float32, trainable=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.math.reduce_mean(tf.math.pow(x, 2), axis=-1, keepdims=True)\n",
    "        output = x * tf.math.rsqrt(x + self.eps) * self.weight\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âˆ†ð‘¡ = ðœâˆ†(ð–¯ð–ºð—‹ð–ºð—†ð–¾ð—ð–¾ð—‹ + ð‘ âˆ†(ð‘¥ð‘¡))\n",
    "= ð—Œð—ˆð–¿ð—ð—‰ð—…ð—Žð—Œ(ð–¯ð–ºð—‹ð–ºð—†ð–¾ð—ð–¾ð—‹ + ð–«ð—‚ð—‡ð–¾ð–ºð—‹(ð‘¥ð‘¡))\n",
    "= ð—Œð—ˆð–¿ð—ð—‰ð—…ð—Žð—Œ(ð–«ð—‚ð—‡ð–¾ð–ºð—‹(ð‘¥ð‘¡))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_scan(u, delta, A, B, C, D):\n",
    "    dA = tf.einsum('bld,dn->bldn', delta, A) # first step of A_bar = exp(Î”A), i.e., Î”A\n",
    "    dB_u = tf.einsum('bld,bld,bln->bldn', delta, u, B)\n",
    "    \n",
    "    dA_cumsum = tf.pad(\n",
    "        dA[:, 1:], [[0, 0], [1, 1], [0, 0], [0, 0]])[:, 1:, :, :]\n",
    "    \n",
    "    dA_cumsum = tf.reverse(dA_cumsum, axis=[1])  # Flip along axis 1\n",
    "    \n",
    "    # Cumulative sum along all the input tokens, parallel prefix sum, calculates dA for all the input tokens parallely\n",
    "    dA_cumsum = tf.math.cumsum(dA_cumsum, axis=1)  \n",
    "    dA_cumsum = tf.exp(dA_cumsum)  # second step of A_bar = exp(Î”A), i.e., exp(Î”A)\n",
    "    \n",
    "    dA_cumsum = tf.reverse(dA_cumsum, axis=[1])  # Flip back along axis 1\n",
    "\n",
    "    x = dB_u * dA_cumsum\n",
    "    x = tf.math.cumsum(x, axis=1)/(dA_cumsum + 1e-12) # 1e-12 to avoid division by 0\n",
    "\n",
    "    y = tf.einsum('bldn,bln->bld', x, C)\n",
    "    \n",
    "    return y + u * D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaBlock(layers.Layer):\n",
    "    def __init__(self, modelargs: ModelArgs, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.args = modelargs\n",
    "        args = modelargs\n",
    "\n",
    "        self.in_projection = layers.Dense(\n",
    "            args.model_internal_dim * 2, \n",
    "            input_shape=(args.model_input_dims,), \n",
    "            use_bias=False)\n",
    "\n",
    "        self.conv1d = layers.Conv1D(\n",
    "            filters=args.model_internal_dim,\n",
    "            use_bias=args.conv_use_bias,\n",
    "            kernel_size=args.conv_kernel_size,\n",
    "            groups=args.model_internal_dim,\n",
    "            data_format='channels_first',\n",
    "            padding='causal'\n",
    "        )\n",
    "\n",
    "        # this layer takes in current token 'x' and outputs the input-specific Î”, B, C (according to S6)\n",
    "        self.x_projection = layers.Dense(\n",
    "            args.delta_t_rank + args.model_states * 2, \n",
    "            use_bias=False)\n",
    "\n",
    "        # this layer projects Î” from delta_t_rank to the mamba internal dimension\n",
    "        self.delta_t_projection = layers.Dense(args.model_internal_dim, \n",
    "                                               input_shape=(args.delta_t_rank,), use_bias=True)\n",
    "\n",
    "        self.A = tf.Variable(repeat(\n",
    "                tf.range(1, args.model_states+1, dtype=tf.float32), \n",
    "                'n -> d n', d=args.model_internal_dim), trainable=False, dtype=tf.float32)\n",
    "        self.A_log = tf.Variable(tf.math.log(self.A), trainable=True, dtype=tf.float32)\n",
    "\n",
    "        self.D = tf.Variable(np.ones(args.model_internal_dim), dtype=tf.float32)\n",
    "\n",
    "        self.out_projection = layers.Dense(\n",
    "            args.model_input_dims, \n",
    "            input_shape=(args.model_internal_dim,), use_bias=args.dense_use_bias)\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"Mamba block forward. This looks the same as Figure 3 in Section 3.4 in the Mamba paper [1].\n",
    "    \n",
    "        Official Implementation:\n",
    "            class Mamba, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L119\n",
    "            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        (batch_size, seq_len, dimension) = x.shape\n",
    "\n",
    "        x_and_res = self.in_projection(x) # shape = (batch, seq_len, 2 * model_internal_dimension)\n",
    "        (x, res) = tf.split(x_and_res, \n",
    "                            [self.args.model_internal_dim, self.args.model_internal_dim], axis=-1)\n",
    "        \n",
    "        x = rearrange(x, 'b l d_in -> b d_in l') \n",
    "        x = self.conv1d(x)[:, :, :seq_len] \n",
    "        x = rearrange(x, 'b d_in l -> b l d_in') \n",
    "        \n",
    "        x = tf.nn.swish(x) \n",
    "        y = self.ssm(x) \n",
    "        y = y * tf.nn.swish(res) # right side of mamba block image\n",
    "        return self.out_projection(y)\n",
    "    \n",
    "    def ssm(self, x):\n",
    "        \"\"\"Runs the SSM. See:\n",
    "            - Algorithm 2 in Section 3.2 in the Mamba paper [1]\n",
    "            - run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
    "\n",
    "        Official Implementation:\n",
    "            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\n",
    "            \n",
    "        \"\"\"\n",
    "        (d_in, n) = self.A_log.shape\n",
    "\n",
    "        # Compute âˆ† A B C D, the state space parameters.\n",
    "        #     A, D are input independent (see Mamba paper [1] Section 3.5.2 \"Interpretation of A\" for why A isn't selective)\n",
    "        #     âˆ†, B, C are input-dependent (this is a key difference between Mamba and the linear time invariant S4,\n",
    "        #                                  and is why Mamba is called **selective** state spaces)\n",
    "\n",
    "        A = -tf.exp(tf.cast(self.A_log, tf.float32)) # shape -> (d_in, n)\n",
    "        D = tf.cast(self.D, tf.float32)\n",
    "\n",
    "        x_dbl = self.x_projection(x) # shape -> (batch, seq_len, delta_t_rank + 2*n)\n",
    "\n",
    "        \n",
    "        (delta, B, C) = tf.split(\n",
    "            x_dbl, \n",
    "            num_or_size_splits=[self.args.delta_t_rank, n, n], \n",
    "            axis=-1) # delta.shape -> (batch, seq_len) & B, C shape -> (batch, seq_len, n)\n",
    "\n",
    "        delta = tf.nn.softplus(\n",
    "            self.delta_t_projection( delta)) # shape -> (batch, seq_len, model_input_dim)\n",
    "\n",
    "        return selective_scan(x, delta, A, B, C, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(layers.Layer):\n",
    "    def __init__(self, modelargs: ModelArgs, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.args = modelargs\n",
    "        args = modelargs\n",
    "\n",
    "        self.mixer = MambaBlock(args)\n",
    "        # self.norm = RMSNorm(args.model_input_dims)\n",
    "        self.norm = layers.LayerNormalization(epsilon=1e-5)\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Official Implementation:\n",
    "            Block.forward(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L297\n",
    "            \n",
    "            Note: the official repo chains residual blocks that look like\n",
    "                [Add -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> ...\n",
    "            where the first Add is a no-op. This is purely for performance reasons as this\n",
    "            allows them to fuse the Add->Norm.\n",
    "\n",
    "            We instead implement our blocks as the more familiar, simpler, and numerically equivalent\n",
    "                [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> ....\n",
    "            \n",
    "        \"\"\"\n",
    "        return self.mixer(self.norm(x)) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def init_model(args: ModelArgs):\n",
    "    input_layer = layers.Input(shape=(args.seq_length,), name='input_ids')\n",
    "    x = layers.Embedding(args.vocab_size, args.model_input_dims, input_length=args.seq_length)(input_layer)\n",
    "\n",
    "    for i in range(args.num_layers):\n",
    "        x = ResidualBlock(args, name=f\"Residual_{i}\")(x)\n",
    "        x = layers.Dropout(args.dropout_rate)(x)\n",
    "\n",
    "    x = layers.LayerNormalization(epsilon=1e-5)(x)\n",
    "\n",
    "    if not args.use_lm_head: # use flatten only if we are using the model as an LM\n",
    "        x = layers.Flatten()(x)\n",
    "    x = layers.Dense(1024, activation=tf.nn.gelu)(x)\n",
    "    output_layer = layers.Dense(args.num_classes, activation=args.final_activation)(x)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer, name='Mamba_ka_Mamba')\n",
    "    model.compile(\n",
    "        loss=args.loss,\n",
    "        optimizer=args.optimizer,\n",
    "        metrics=args.metrics\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset = load_dataset(\"ajaykarthick/imdb-movie-reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ModelArgs(\n",
    "    model_input_dims=128,\n",
    "    model_states=32,\n",
    "    num_layers=12,\n",
    "    dropout_rate=0.2,\n",
    "    vocab_size=vocab_size,\n",
    "    num_classes=1,\n",
    "    loss='binary_crossentropy',\n",
    ")\n",
    "model = init_model(args)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, test_labels = [], []\n",
    "train_ids = np.zeros((len(dataset['train']), args.seq_length))\n",
    "test_ids = np.zeros((len(dataset['test']), args.seq_length))\n",
    "\n",
    "for i, item in enumerate(tqdm(dataset['train'])):\n",
    "    text = item['review']\n",
    "    train_ids[i, :] = tokenizer.encode_plus(text, max_length=args.seq_length, padding='max_length', return_tensors='np')['input_ids'][0][:args.seq_length]\n",
    "    train_labels.append(item['label'])\n",
    "\n",
    "for i, item in enumerate(tqdm(dataset['test'])):\n",
    "    text = item['review']\n",
    "    test_ids[i, :] = tokenizer.encode_plus(text, max_length=args.seq_length, padding='max_length', return_tensors='np')['input_ids'][0][:args.seq_length]\n",
    "    test_labels.append(item['label'])\n",
    "del dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_ids, train_labels)).batch(BATCH_SIZE).shuffle(1000)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_ids, test_labels)).batch(BATCH_SIZE).shuffle(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, validation_data=test_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(text: str, model: Model, tokenizer):\n",
    "    tokens = tokenizer.encode(text, max_length=args.seq_length, padding='max_length', return_tensors='np')\n",
    "    output = model(tokens)[-1, 0]\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(infer(\"Hello what is up\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
